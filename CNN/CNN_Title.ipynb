{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5b88e8e-e454-49d2-ad8e-8e63defc99ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8.10 (tags/v3.8.10:3d8993a, May  3 2021, 11:48:03) [MSC v.1928 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "%%script C:\\Users\\Jan Catherine\\AppData\\Local\\Programs\\Python\\Python38\\python.exe\n",
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6882c84-4ec7-4025-862c-e30f906994de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Jan Catherine\\\\Documents\\\\CMSC Notes\\\\CMSC 190 Part 2\\\\Codes\\\\CNN'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57500b2d-3257-4427-9f33-9d055fc284e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../../Data_Collection_02/02_Preprocessing/05_CNN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8863d932-63b8-4c2b-a9bf-cc584f2b6830",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc5f248c-54b6-4365-af00-0637fb458e42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom keras.models import Sequential\\nfrom keras.layers import Dense, Dropout, Activation, Flatten\\nfrom keras.layers import Convolution2D, MaxPooling2D\\nfrom sklearn.model_selection import train_test_split\\nimport pandas as pd\\nimport numpy as np\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing libraries\n",
    "\"\"\"\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61db8ea8-a355-4d58-b625-4013889267b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import string\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from tensorflow.keras import preprocessing\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "#import pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b31ffad0-8c40-4997-91a8-c775cb33d04d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random over-sampling:\n",
      "0    5760\n",
      "1    5760\n",
      "Name: Fake, dtype: int64\n",
      "0    5760\n",
      "1    5760\n",
      "Name: Fake, dtype: int64\n",
      "-------Train data--------\n",
      "0    5760\n",
      "1    5760\n",
      "Name: Fake, dtype: int64\n",
      "11520\n",
      "-------------------------\n",
      "-------Test data--------\n",
      "0    1428\n",
      "1     184\n",
      "Name: Fake, dtype: int64\n",
      "1612\n",
      "-------------------------\n",
      "Train Max Length :20\n",
      "Test Max Sentence Length :17\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv('01_Title_Train.csv', encoding='latin-1')\n",
    "testdata = pd.read_csv('01_Title_Test.csv', encoding='latin-1')\n",
    "\n",
    "train_data = dataset\n",
    "test_data = testdata\n",
    "\n",
    "#Oversampling\n",
    "count_class_0, count_class_1 = train_data.Fake.value_counts()\n",
    "\n",
    "df_class_0 = train_data[train_data['Fake'] == 0]\n",
    "df_class_1 = train_data[train_data['Fake'] == 1]\n",
    "\n",
    "df_class_1_over = df_class_1.sample(count_class_0, replace=True)\n",
    "df_test_over = pd.concat([df_class_0, df_class_1_over], axis=0)\n",
    "\n",
    "df_test_over = df_test_over.reset_index(drop=True)\n",
    "print('Random over-sampling:')\n",
    "print(df_test_over.Fake.value_counts())\n",
    "\n",
    "train_data = df_test_over\n",
    "#train_data['Title'] = df_test_over['Title']\n",
    "#train_data['Fake'] = df_test_over['Fake']\n",
    "\n",
    "print(train_data.Fake.value_counts())\n",
    "\n",
    "train_data.dropna(axis=0, how ='any', inplace=True)\n",
    "train_data['Num_words_title'] = train_data['Title'].apply(lambda x:len(str(x).split()))\n",
    "#train_data['Num_words_desc'] = train_data['Description'].apply(lambda x:len(str(x).split()))\n",
    "print('-------Train data--------')\n",
    "print(train_data['Fake'].value_counts())\n",
    "print(len(train_data))\n",
    "print('-------------------------')\n",
    "\n",
    "max_train_title_length  = train_data['Num_words_title'].max()\n",
    "#max_train_desc_length  = train_data['Num_words_desc'].max()\n",
    "\n",
    "\n",
    "test_data.dropna(axis = 0, how ='any',inplace=True) \n",
    "test_data['Num_words_title'] = test_data['Title'].apply(lambda x:len(str(x).split())) \n",
    "\n",
    "max_test_title_length  = test_data['Num_words_title'].max()\n",
    "\n",
    "print('-------Test data--------')\n",
    "print(test_data['Fake'].value_counts())\n",
    "print(len(test_data))\n",
    "print('-------------------------')\n",
    "\n",
    "print('Train Max Length :'+str(max_train_title_length))\n",
    "print('Test Max Sentence Length :'+str(max_test_title_length))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79a2bbef-3563-46e8-a7b6-960806b53caf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Fake</th>\n",
       "      <th>Num_words_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>key arsenal rural India pandemic fight</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Coronavirus banks : Implications leaders</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nail Salons Massage Parlors Tattoo Studios</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Risk SARSCoV2 transmission aerosols rational</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>House prices Spain : COVID19 affected idealista</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Title  Fake  Num_words_title\n",
       "0           key arsenal rural India pandemic fight     0                6\n",
       "1         Coronavirus banks : Implications leaders     0                5\n",
       "2       Nail Salons Massage Parlors Tattoo Studios     0                6\n",
       "3     Risk SARSCoV2 transmission aerosols rational     0                5\n",
       "4  House prices Spain : COVID19 affected idealista     0                7"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21264bed-6b54-438e-b82e-8fe0913d7278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5, 57, 21, 1]]\n"
     ]
    }
   ],
   "source": [
    "num_words = 20000\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(num_words=num_words, oov_token=\"unk\")\n",
    "tokenizer.fit_on_texts(train_data['Title'].tolist())\n",
    "\n",
    "print(str(tokenizer.texts_to_sequences(['5G Global bill airfare'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c10b99-c9bf-4424-91c1-a03bd4412176",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f7afecf-1b5d-4343-abe8-91fa964c335b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"word_index = tokenizer.word_index\\nvocab_size = len(word_index) + 1\\nprint(f'Vocabulary Size : {vocab_size}')\\n\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"word_index = tokenizer.word_index\n",
    "vocab_size = len(word_index) + 1\n",
    "print(f'Vocabulary Size : {vocab_size}')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61fbc624-d2a4-41b1-be0a-e7683b4d2b39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data len:9216\n",
      "Class distributionCounter({0: 4608, 1: 4608})\n",
      "Valid data len:2304\n",
      "Class distributionCounter({1: 1152, 0: 1152})\n",
      "[6186  260  520    4   25   61 2647    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jan Catherine\\AppData\\Local\\Temp\\ipykernel_7328\\740212987.py:12: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  x_train = np.array( tokenizer.texts_to_sequences(X_train) )\n",
      "C:\\Users\\Jan Catherine\\AppData\\Local\\Temp\\ipykernel_7328\\740212987.py:13: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  x_valid = np.array( tokenizer.texts_to_sequences(X_valid) )\n",
      "C:\\Users\\Jan Catherine\\AppData\\Local\\Temp\\ipykernel_7328\\740212987.py:14: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  x_test  = np.array( tokenizer.texts_to_sequences(test_data['Title'].tolist()) )\n"
     ]
    }
   ],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(train_data['Title'].tolist(),\\\n",
    "                                                      train_data['Fake'].tolist(),\\\n",
    "                                                      test_size=0.2,\\\n",
    "                                                      stratify = train_data['Fake'].tolist(),\\\n",
    "                                                      random_state=0)\n",
    "\n",
    "print('Train data len:'+str(len(X_train)))\n",
    "print('Class distribution'+str(Counter(y_train)))\n",
    "print('Valid data len:'+str(len(X_valid)))\n",
    "print('Class distribution'+ str(Counter(y_valid)))\n",
    "\n",
    "x_train = np.array( tokenizer.texts_to_sequences(X_train) )\n",
    "x_valid = np.array( tokenizer.texts_to_sequences(X_valid) )\n",
    "x_test  = np.array( tokenizer.texts_to_sequences(test_data['Title'].tolist()) )\n",
    "\n",
    "x_train = pad_sequences(x_train, padding='post', maxlen=90)\n",
    "x_valid = pad_sequences(x_valid, padding='post', maxlen=90)\n",
    "x_test = pad_sequences(x_test, padding='post', maxlen=90)\n",
    "\n",
    "print(x_train[0])\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "train_labels = le.fit_transform(y_train)\n",
    "train_labels = np.asarray( tf.keras.utils.to_categorical(train_labels, 2))\n",
    "#print(train_labels)\n",
    "valid_labels = le.transform(y_valid)\n",
    "valid_labels = np.asarray( tf.keras.utils.to_categorical(valid_labels, 2))\n",
    "\n",
    "\n",
    "test_labels = le.transform(test_data['Fake'].tolist())\n",
    "test_labels = np.asarray(tf.keras.utils.to_categorical(test_labels, 2))\n",
    "list(le.classes_)\n",
    "\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((x_train,train_labels))\n",
    "valid_ds = tf.data.Dataset.from_tensor_slices((x_valid,valid_labels))\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((x_test,test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0143d15f-85f2-4690-8a64-e438f3f4656a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Fake</th>\n",
       "      <th>Num_words_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Coronavirus fact check : recycled air planes</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Coronavirus Italy travel advice : safe travel ...</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Wayne Rooney pays tribute wife Coleen wedding ...</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Trump blame game China disguise massive COVID1...</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>COVID19 Testing Locations</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  Fake  Num_words_title\n",
       "0       Coronavirus fact check : recycled air planes     0                7\n",
       "1  Coronavirus Italy travel advice : safe travel ...     0                9\n",
       "2  Wayne Rooney pays tribute wife Coleen wedding ...     0               11\n",
       "3  Trump blame game China disguise massive COVID1...     0               10\n",
       "4                          COVID19 Testing Locations     0                3"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d9051ccc-ca32-4452-84f3-a0d7cb005e13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"print(y_train[:10])\\ntrain_labels = le.fit_transform(y_train)\\nprint('Text to number')\\nprint(train_labels[:10])\\ntrain_labels = np.asarray( tf.keras.utils.to_categorical(train_labels))\\nprint('Number to category')\\nprint(train_labels[:10])\\n\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"print(y_train[:10])\n",
    "train_labels = le.fit_transform(y_train)\n",
    "print('Text to number')\n",
    "print(train_labels[:10])\n",
    "train_labels = np.asarray( tf.keras.utils.to_categorical(train_labels))\n",
    "print('Number to category')\n",
    "print(train_labels[:10])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "26bf38b8-3b43-49a7-a1b8-90da4888318e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======Train dataset ====\n",
      "tf.Tensor(\n",
      "[6186  260  520    4   25   61 2647    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0], shape=(90,), dtype=int32) tf.Tensor([1. 0.], shape=(2,), dtype=float32)\n",
      "tf.Tensor(\n",
      "[ 95 282 213 292 824   7   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0], shape=(90,), dtype=int32) tf.Tensor([1. 0.], shape=(2,), dtype=float32)\n",
      "tf.Tensor(\n",
      "[   2 2309   76 3853 6119 2125    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0], shape=(90,), dtype=int32) tf.Tensor([1. 0.], shape=(2,), dtype=float32)\n",
      "======Validation dataset ====\n",
      "tf.Tensor(\n",
      "[246  56 145 251 252   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0], shape=(90,), dtype=int32) tf.Tensor([0. 1.], shape=(2,), dtype=float32)\n",
      "tf.Tensor(\n",
      "[ 10  70  71 278  80 375  18   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0], shape=(90,), dtype=int32) tf.Tensor([0. 1.], shape=(2,), dtype=float32)\n",
      "tf.Tensor(\n",
      "[ 266   94  334  313 3085 3443    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0], shape=(90,), dtype=int32) tf.Tensor([1. 0.], shape=(2,), dtype=float32)\n",
      "======Test dataset ====\n",
      "tf.Tensor(\n",
      "[   2  166  283 6572   58 2869    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0], shape=(90,), dtype=int32) tf.Tensor([1. 0.], shape=(2,), dtype=float32)\n",
      "tf.Tensor(\n",
      "[  2  12  20 143 135  20  12   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0], shape=(90,), dtype=int32) tf.Tensor([1. 0.], shape=(2,), dtype=float32)\n",
      "tf.Tensor(\n",
      "[5536 5537 2787 3712 2759 5538 2835 5539   36  271 5540    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0], shape=(90,), dtype=int32) tf.Tensor([1. 0.], shape=(2,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "count =0\n",
    "print('======Train dataset ====')\n",
    "for value,label in train_ds:\n",
    "    count += 1\n",
    "    print(value,label)\n",
    "    if count==3:\n",
    "        break\n",
    "count =0\n",
    "print('======Validation dataset ====')\n",
    "for value,label in valid_ds:\n",
    "    count += 1\n",
    "    print(value,label)\n",
    "    if count==3:\n",
    "        break\n",
    "count = 0\n",
    "print('======Test dataset ====')\n",
    "for value,label in test_ds:\n",
    "    count += 1\n",
    "    print(value,label)\n",
    "    if count==3:\n",
    "        break  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "95dc6838-cfe5-4647-8591-125720406e10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 90, 64)            750016    \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 88, 128)           24704     \n",
      "                                                                 \n",
      " global_max_pooling1d (Glob  (None, 128)               0         \n",
      " alMaxPooling1D)                                                 \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 2)                 258       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 774978 (2.96 MB)\n",
      "Trainable params: 774978 (2.96 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "111/288 [==========>...................] - ETA: 4s - loss: 0.7528 - categorical_accuracy: 0.7123"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_7328\\2549686204.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[1;31m# Fit the model using the train and test datasets.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[1;31m#history = model.fit(x_train, train_labels,validation_data= (x_test,test_labels),epochs=epochs )\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m     history = model.fit(inputs[train], targets[train],\n\u001b[0m\u001b[0;32m     51\u001b[0m                         \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m                         verbose=1)\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1740\u001b[0m                         ):\n\u001b[0;32m   1741\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1742\u001b[1;33m                             \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1743\u001b[0m                             \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1744\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    823\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    824\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 825\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    826\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    855\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    856\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 857\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_no_variable_creation_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    858\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    859\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    146\u001b[0m       (concrete_function,\n\u001b[0;32m    147\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m--> 148\u001b[1;33m     return concrete_function._call_flat(\n\u001b[0m\u001b[0;32m    149\u001b[0m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m    150\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs)\u001b[0m\n\u001b[0;32m   1347\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1348\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1349\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_build_call_outputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1350\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1351\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    194\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m           \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[0;32m    197\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\context.py\u001b[0m in \u001b[0;36mcall_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1455\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1456\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1457\u001b[1;33m       outputs = execute.execute(\n\u001b[0m\u001b[0;32m   1458\u001b[0m           \u001b[0mname\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"utf-8\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1459\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#CNN MODEL\n",
    "\n",
    "max_features = 11718\n",
    "embedding_dim = 64 #same as URLNet\n",
    "sequence_length = 90\n",
    "\n",
    "# Define per-fold score containers\n",
    "acc_per_fold = []\n",
    "loss_per_fold = []\n",
    "\n",
    "inputs = np.concatenate((x_train,x_valid), axis=0)\n",
    "targets = np.concatenate((train_labels, valid_labels), axis=0)\n",
    "\n",
    "#regularizer prevents overfitting\n",
    "# Define the K-fold Cross Validator\n",
    "kfold = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "# K-fold Cross Validation model evaluation\n",
    "fold_no = 1\n",
    "for train, test in kfold.split(inputs, targets):\n",
    "    \n",
    "    \n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Embedding(max_features +1, embedding_dim, input_length=sequence_length,\\\n",
    "                                        embeddings_regularizer = regularizers.l2(0.0005)))                                    \n",
    "\n",
    "    model.add(tf.keras.layers.Conv1D(128,3, activation='relu',\\\n",
    "                                     kernel_regularizer = regularizers.l2(0.0005),\\\n",
    "                                     bias_regularizer = regularizers.l2(0.0005)))                               \n",
    "\n",
    "\n",
    "    model.add(tf.keras.layers.GlobalMaxPooling1D())\n",
    "\n",
    "    model.add(tf.keras.layers.Dropout(0.5)) #to reduce overfitting\n",
    "\n",
    "    #final classification, 2 classes\n",
    "    model.add(tf.keras.layers.Dense(2, activation='sigmoid',\\\n",
    "                                    kernel_regularizer=regularizers.l2(0.001),\\\n",
    "                                    bias_regularizer=regularizers.l2(0.001),))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    model.summary()\n",
    "    model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=False), optimizer='Nadam', metrics=[\"CategoricalAccuracy\"])\n",
    "    \n",
    "    epochs = 100\n",
    "    # Fit the model using the train and test datasets.\n",
    "    #history = model.fit(x_train, train_labels,validation_data= (x_test,test_labels),epochs=epochs )\n",
    "    history = model.fit(inputs[train], targets[train],\n",
    "                        epochs= epochs ,\n",
    "                        verbose=1)\n",
    "    # Generate generalization metrics\n",
    "    scores = model.evaluate(inputs[test], targets[test], verbose=0)\n",
    "    print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
    "    acc_per_fold.append(scores[1] * 100)\n",
    "    loss_per_fold.append(scores[0])\n",
    "\n",
    "    # Increase fold number\n",
    "    fold_no = fold_no + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4aa2aa-6502-4e52-bffb-e5d32167ae37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == Provide average scores ==\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Score per fold')\n",
    "for i in range(0, len(acc_per_fold)):\n",
    "  print('------------------------------------------------------------------------')\n",
    "  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Average scores for all folds:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
    "print(f'> Loss: {np.mean(loss_per_fold)}')\n",
    "print('------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "27b715be-0b02-486a-b13c-e63f567aa4c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [0.5477079153060913,\n",
       "  0.22543710470199585,\n",
       "  0.1701410859823227,\n",
       "  0.15694795548915863,\n",
       "  0.15227153897285461,\n",
       "  0.1496129035949707,\n",
       "  0.15003007650375366,\n",
       "  0.1484369933605194,\n",
       "  0.14846058189868927,\n",
       "  0.14879468083381653,\n",
       "  0.14865358173847198,\n",
       "  0.14683188498020172,\n",
       "  0.14828363060951233,\n",
       "  0.1484348177909851,\n",
       "  0.1464097648859024,\n",
       "  0.14738918840885162,\n",
       "  0.14583763480186462,\n",
       "  0.14640407264232635,\n",
       "  0.1456795632839203,\n",
       "  0.14453111588954926,\n",
       "  0.14725655317306519,\n",
       "  0.14615312218666077,\n",
       "  0.14414525032043457,\n",
       "  0.14563769102096558,\n",
       "  0.14506112039089203,\n",
       "  0.14394523203372955,\n",
       "  0.14446957409381866,\n",
       "  0.14244969189167023,\n",
       "  0.14285022020339966,\n",
       "  0.1426951140165329,\n",
       "  0.14380700886249542,\n",
       "  0.14237645268440247,\n",
       "  0.1410638988018036,\n",
       "  0.1413307785987854,\n",
       "  0.14065681397914886,\n",
       "  0.14046092331409454,\n",
       "  0.14100152254104614,\n",
       "  0.1404103934764862,\n",
       "  0.1391919106245041,\n",
       "  0.13926194608211517,\n",
       "  0.1405961513519287,\n",
       "  0.13847416639328003,\n",
       "  0.1386929303407669,\n",
       "  0.13923844695091248,\n",
       "  0.13959258794784546,\n",
       "  0.13793984055519104,\n",
       "  0.13789337873458862,\n",
       "  0.13847318291664124,\n",
       "  0.1372295469045639,\n",
       "  0.13732405006885529,\n",
       "  0.13771787285804749,\n",
       "  0.13731180131435394,\n",
       "  0.13696688413619995,\n",
       "  0.13756155967712402,\n",
       "  0.13727840781211853,\n",
       "  0.13697481155395508,\n",
       "  0.13754583895206451,\n",
       "  0.13664600253105164,\n",
       "  0.13646544516086578,\n",
       "  0.1362352967262268,\n",
       "  0.13620294630527496,\n",
       "  0.1371147334575653,\n",
       "  0.13594844937324524,\n",
       "  0.1358950287103653,\n",
       "  0.13708749413490295,\n",
       "  0.13555602729320526,\n",
       "  0.13754324615001678,\n",
       "  0.1360732764005661,\n",
       "  0.13501973450183868,\n",
       "  0.1367061734199524,\n",
       "  0.13599199056625366,\n",
       "  0.1347230225801468,\n",
       "  0.13550284504890442,\n",
       "  0.13578566908836365,\n",
       "  0.13467487692832947,\n",
       "  0.1347689926624298,\n",
       "  0.1351497620344162,\n",
       "  0.1349642425775528,\n",
       "  0.13523262739181519,\n",
       "  0.13484878838062286,\n",
       "  0.13548272848129272,\n",
       "  0.13388317823410034,\n",
       "  0.13456997275352478,\n",
       "  0.13448740541934967,\n",
       "  0.13471746444702148,\n",
       "  0.13488031923770905,\n",
       "  0.13492268323898315,\n",
       "  0.13516846299171448,\n",
       "  0.13430388271808624,\n",
       "  0.13316458463668823,\n",
       "  0.13389131426811218,\n",
       "  0.13380169868469238,\n",
       "  0.13476698100566864,\n",
       "  0.1335279792547226,\n",
       "  0.13426218926906586,\n",
       "  0.13395823538303375,\n",
       "  0.13507230579853058,\n",
       "  0.13406707346439362,\n",
       "  0.13306984305381775,\n",
       "  0.13302692770957947],\n",
       " 'categorical_accuracy': [0.8084852695465088,\n",
       "  0.9671223759651184,\n",
       "  0.9900173544883728,\n",
       "  0.9938151240348816,\n",
       "  0.9954426884651184,\n",
       "  0.9950087070465088,\n",
       "  0.9951171875,\n",
       "  0.9951171875,\n",
       "  0.9964192509651184,\n",
       "  0.9958767294883728,\n",
       "  0.9956597089767456,\n",
       "  0.99609375,\n",
       "  0.9953342080116272,\n",
       "  0.9959852695465088,\n",
       "  0.9964192509651184,\n",
       "  0.9967448115348816,\n",
       "  0.9965277910232544,\n",
       "  0.9967448115348816,\n",
       "  0.9968532919883728,\n",
       "  0.9973958134651184,\n",
       "  0.9959852695465088,\n",
       "  0.9959852695465088,\n",
       "  0.9968532919883728,\n",
       "  0.9957682490348816,\n",
       "  0.99609375,\n",
       "  0.9963107705116272,\n",
       "  0.9959852695465088,\n",
       "  0.9971787929534912,\n",
       "  0.9966362714767456,\n",
       "  0.9966362714767456,\n",
       "  0.9964192509651184,\n",
       "  0.9972873330116272,\n",
       "  0.9969618320465088,\n",
       "  0.9976128339767456,\n",
       "  0.9968532919883728,\n",
       "  0.9977213740348816,\n",
       "  0.9971787929534912,\n",
       "  0.9968532919883728,\n",
       "  0.9971787929534912,\n",
       "  0.9977213740348816,\n",
       "  0.9967448115348816,\n",
       "  0.9975043535232544,\n",
       "  0.9971787929534912,\n",
       "  0.9967448115348816,\n",
       "  0.998046875,\n",
       "  0.9976128339767456,\n",
       "  0.9973958134651184,\n",
       "  0.9973958134651184,\n",
       "  0.9979383945465088,\n",
       "  0.9981553554534912,\n",
       "  0.9977213740348816,\n",
       "  0.9979383945465088,\n",
       "  0.998046875,\n",
       "  0.9975043535232544,\n",
       "  0.9977213740348816,\n",
       "  0.9971787929534912,\n",
       "  0.9975043535232544,\n",
       "  0.9975043535232544,\n",
       "  0.9976128339767456,\n",
       "  0.9979383945465088,\n",
       "  0.9975043535232544,\n",
       "  0.9977213740348816,\n",
       "  0.9978298544883728,\n",
       "  0.998046875,\n",
       "  0.998046875,\n",
       "  0.9977213740348816,\n",
       "  0.9970703125,\n",
       "  0.9981553554534912,\n",
       "  0.9972873330116272,\n",
       "  0.9973958134651184,\n",
       "  0.9979383945465088,\n",
       "  0.9981553554534912,\n",
       "  0.9977213740348816,\n",
       "  0.9976128339767456,\n",
       "  0.998046875,\n",
       "  0.9978298544883728,\n",
       "  0.9979383945465088,\n",
       "  0.998046875,\n",
       "  0.998046875,\n",
       "  0.9977213740348816,\n",
       "  0.9976128339767456,\n",
       "  0.9982638955116272,\n",
       "  0.9981553554534912,\n",
       "  0.9975043535232544,\n",
       "  0.9978298544883728,\n",
       "  0.9983723759651184,\n",
       "  0.9971787929534912,\n",
       "  0.9976128339767456,\n",
       "  0.9978298544883728,\n",
       "  0.9982638955116272,\n",
       "  0.9977213740348816,\n",
       "  0.998046875,\n",
       "  0.9979383945465088,\n",
       "  0.9981553554534912,\n",
       "  0.9983723759651184,\n",
       "  0.9979383945465088,\n",
       "  0.9981553554534912,\n",
       "  0.9977213740348816,\n",
       "  0.9977213740348816,\n",
       "  0.9982638955116272]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6fc01b7d-a7ab-4284-964d-68598a93d134",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'plt.plot(history.history[\\'loss\\'], label=\\' training data\\')\\nplt.plot(history.history[\\'val_loss\\'], label=\\'validation data\\')\\nplt.title(\\'Loss for Text Classification\\')\\nplt.ylabel(\\'Loss value\\')\\nplt.xlabel(\\'No. epoch\\')\\nplt.legend(loc=\"upper left\")\\nplt.show()\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"plt.plot(history.history['loss'], label=' training data')\n",
    "plt.plot(history.history['val_loss'], label='validation data')\n",
    "plt.title('Loss for Text Classification')\n",
    "plt.ylabel('Loss value')\n",
    "plt.xlabel('No. epoch')\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6006641d-83c9-425e-b84d-9451bbd391d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'plt.plot(history.history[\\'categorical_accuracy\\'], label=\\' (training data)\\')\\nplt.plot(history.history[\\'val_categorical_accuracy\\'], label=\\'CategoricalCrossentropy (validation data)\\')\\nplt.title(\\'CategoricalAccuracy for Text Classification\\')\\nplt.ylabel(\\'CategoricalAccuracy value\\')\\nplt.xlabel(\\'No. epoch\\')\\nplt.legend(loc=\"upper left\")\\nplt.show()\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"plt.plot(history.history['categorical_accuracy'], label=' (training data)')\n",
    "plt.plot(history.history['val_categorical_accuracy'], label='CategoricalCrossentropy (validation data)')\n",
    "plt.title('CategoricalAccuracy for Text Classification')\n",
    "plt.ylabel('CategoricalAccuracy value')\n",
    "plt.xlabel('No. epoch')\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "655a8265-35fb-47f6-823c-f9269066429d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    layer._name = layer.name + str(\"_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "71d38779-c46b-4600-aaa7-752e4d5f6120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\Jan Catherine\\Documents\\CMSC Notes\\CMSC 190 Part 2\\Codes\\CNN\\Title\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\Jan Catherine\\Documents\\CMSC Notes\\CMSC 190 Part 2\\Codes\\CNN\\Title\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save('C:\\\\Users\\\\Jan Catherine\\\\Documents\\\\CMSC Notes\\\\CMSC 190 Part 2\\\\Codes\\\\CNN\\\\Title') \n",
    "json_string1 = tokenizer.to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c07f08ee-072a-494b-9587-97c719629c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('C:\\\\Users\\\\Jan Catherine\\\\Documents\\\\CMSC Notes\\\\CMSC 190 Part 2\\\\Codes\\\\CNN\\\\Title\\\\Title.json', 'w') as outfile1:\n",
    "    json.dump(json_string1, outfile1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "093fad61-1d01-4441-af0d-3e758dcfd3f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_4_1 (Embedding)   (None, 90, 64)            750016    \n",
      "                                                                 \n",
      " conv1d_4_1 (Conv1D)         (None, 88, 128)           24704     \n",
      "                                                                 \n",
      " global_max_pooling1d_4_1 (  (None, 128)               0         \n",
      " GlobalMaxPooling1D)                                             \n",
      "                                                                 \n",
      " dropout_4_1 (Dropout)       (None, 128)               0         \n",
      "                                                                 \n",
      " dense_4_1 (Dense)           (None, 2)                 258       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 774978 (2.96 MB)\n",
      "Trainable params: 774978 (2.96 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#reload model\n",
    "\n",
    "new_model = tf.keras.models.load_model('C:\\\\Users\\\\Jan Catherine\\\\Documents\\\\CMSC Notes\\\\CMSC 190 Part 2\\\\Codes\\\\CNN\\\\Title')\n",
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4aa4ad50-70d5-491a-86a6-c0ec45ac1e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('C:\\\\Users\\\\Jan Catherine\\\\Documents\\\\CMSC Notes\\\\CMSC 190 Part 2\\\\Codes\\\\CNN\\\\Title\\\\Title.json') as json_file1:\n",
    "    json_string1 = json.load(json_file1)\n",
    "tokenizer1 = tf.keras.preprocessing.text.tokenizer_from_json(json_string1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0593ab06-c7f7-45ad-99d3-0bb97114bad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jan Catherine\\AppData\\Local\\Temp\\ipykernel_17704\\3760519126.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  x_test  = np.array( tokenizer1.texts_to_sequences(test_data['Title'].tolist()) )\n"
     ]
    }
   ],
   "source": [
    "x_test  = np.array( tokenizer1.texts_to_sequences(test_data['Title'].tolist()) )\n",
    "x_test = pad_sequences(x_test, padding='post', maxlen=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "285b5d0c-c6cc-4ad3-851c-d0dda819352f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate predictions for all samples\n",
      "51/51 [==============================] - 0s 3ms/step\n",
      "[[0.9986129  0.00138709]\n",
      " [0.99433863 0.00566135]\n",
      " [0.99058545 0.00941452]\n",
      " ...\n",
      " [0.00372173 0.9962783 ]\n",
      " [0.9556978  0.04430221]\n",
      " [0.9832606  0.01673941]]\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions (probabilities -- the output of the last layer)\n",
    "# on test  data using `predict`\n",
    "print(\"Generate predictions for all samples\")\n",
    "predictions = new_model.predict(x_test)\n",
    "print(predictions)\n",
    "predict_results = predictions.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a1d03231-1179-4974-8e41-a79d81b9e960",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['pred_fake']= predict_results\n",
    "test_data['pred_fake'] = np.where((test_data.pred_fake == '0'),0,test_data.pred_fake)\n",
    "test_data['pred_fake'] = np.where((test_data.pred_fake == '1'),1,test_data.pred_fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "31a747c5-d56b-41b2-b46f-ddef6c577fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    0\n",
       "2    0\n",
       "3    0\n",
       "4    0\n",
       "Name: Fake, dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(test_data['Fake']))\n",
    "test_data['Fake'].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "57e92601-5e63-4df0-b3b3-6b2fdcb5902c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    0\n",
       "2    0\n",
       "3    0\n",
       "4    0\n",
       "Name: pred_fake, dtype: int64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(test_data['pred_fake']))\n",
    "test_data['pred_fake'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f32efb08-5b28-4e69-8718-4bd1d01411ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.97      0.97      1428\n",
      "           1       0.76      0.71      0.73       184\n",
      "\n",
      "    accuracy                           0.94      1612\n",
      "   macro avg       0.86      0.84      0.85      1612\n",
      "weighted avg       0.94      0.94      0.94      1612\n",
      "\n"
     ]
    }
   ],
   "source": [
    "labels = [0, 1]\n",
    "    \n",
    "print(classification_report(test_data['Fake'].tolist(),test_data['pred_fake'].tolist(),labels=labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed79082-76d1-4346-932c-e970cd4fee8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"precisionscore = precision_score(y_test, new_model.predict(x_test))\n",
    "accuracyscore = accuracy_score(y_test, new_model.predict(x_test))\n",
    "recallscore = recall_score(y_test, new_model.predict(x_test))\n",
    "f1score = f1_score(y_test, new_model.predict(x_test))\n",
    "cm = confusion_matrix(y_test, new_model.predict(x_test))\n",
    "\n",
    "print(precisionscore, accuracyscore, recallscore, f1score, cm)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b9202e-a347-4243-a327-1e3432c9e19d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac01755-1aa1-41d5-8069-c8a0b390ea45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
